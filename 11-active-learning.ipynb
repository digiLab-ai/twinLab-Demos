{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports\n",
    "import twinlab as tl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establishing parameters and training data for an example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we want to establish data and model parameters, and also generate trial data in order to showcase the functionality of active learning (Bayesian optimisation for design of experiments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish data and model parameters\n",
    "dataset_id = \"active-learning\"\n",
    "campaign_id = dataset_id\n",
    "err_sig = 0.25\n",
    "n_train = 100\n",
    "n_eval = 101\n",
    "random_seed = 42\n",
    "n_cycle = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the random-number generator\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Create a sine-wave function to generate training data\n",
    "def f(x):\n",
    "    return np.sin(2*np.pi*x*n_cycle)\n",
    "\n",
    "def model(X):\n",
    "    return np.random.normal(f(X), err_sig)\n",
    "\n",
    "# Set up training data dataframe \n",
    "X = np.array([0.05, 0.1, 0.15, 0.4, 0.45, 0.48, 0.53, 0.85, 0.9, 0.95])\n",
    "y = model(X)\n",
    "df_train = pd.DataFrame({\"X\": X, \"y\": y})\n",
    "display(df_train)\n",
    "\n",
    "# Send the training dataset to the cloud\n",
    "tl.upload_dataset(df_train, dataset_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation data for testing of the model after it's been trained\n",
    "eval = {\"X\": np.linspace(0, 1, n_eval)}\n",
    "df_test = pd.DataFrame(eval)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting routuine\n",
    "def plot_data(df, grid, mean, stdv, xs=None):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(grid, mean, \"-\", label=\"Pre-trained model\")\n",
    "    if xs is not None:\n",
    "        for x in xs:\n",
    "            label = \"Recommended points\" if x == xs[-1] else None\n",
    "            transform = ax.get_xaxis_transform()\n",
    "            plt.arrow(*x, 0.2, 0., -0.13, color='grey', head_width=0.02, head_length=0.05, alpha=0.8, transform=transform)\n",
    "    for nsig in [1,2]:\n",
    "        label = \"Model uncertainty\" if nsig == 1 else None\n",
    "        plt.fill_between(grid, mean-nsig*stdv, mean+nsig*stdv, lw=0, alpha=0.25, color=\"C0\", label=label)\n",
    "    ax.plot(df[\"X\"], df[\"y\"], \".\", color=\"black\", label=\"Datapoints\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll be establishing the model training parameters, training our model in the Cloud, and producing the output of that training and prediction. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to establish what criteria our model training has. We do this via a JSON dictionary, which will include what we want to name our model, specify what the model inputs and outputs are, and fine-tune modelling parameters, such as the test_train_ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "params = {\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"inputs\" : [\"X\"],\n",
    "    \"outputs\": [\"y\"],\n",
    "    \"test_train_ratio\": 1.,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can actually train our model in the Cloud, and pass through predict_campaign, too, which will make predictions from this model that we have trained using the test data we have set up. From this function we can get back our mean predictions, as well as the predictions for the standard deviations. This quantifies the uncertainty in our trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model \n",
    "tl.train_campaign(params, campaign_id, verbose=True)\n",
    "\n",
    "# Predict \n",
    "df_mean, df_stdv = tl.predict_campaign(df_test, campaign_id)\n",
    "mean, stdv = df_mean[\"y\"].values, df_stdv[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameters \n",
    "grid = df_test[\"X\"].values\n",
    "# alpha_fill = 0.25\n",
    "# nrow, ncol = 2, 2\n",
    "# figx, figy = 4, 3\n",
    "\n",
    "# # Plot the trained model\n",
    "plot_data(df_train, grid, mean, stdv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above graph our data points in the black, the model that has been generated based on those points (the blue line), and as well the predicted deviation from that model (depicted in shades of 1 sigma and 2 sigma around the blue model function). Our next question might then be--to improve this model, where should we next take more data? As you can see, there's a wide range of x's--but which x's should we model to generate the best possible improvement in our model, and reduce those sigmas? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer those questions, we need to use active learning. This is part of a wider research area called design of experiments, and in twinLab we particularly implement this as a form of Bayesian optimisation which we call active learning. Currently on the API, we have implemented active learning using a specific acquisition function, a Monte Carlo instance of Negative Integrated Posterior Variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use active learning \n",
    "df_active = tl.active_learn_campaign(campaign_id, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trained model and test data \n",
    "plot_data(df_train, grid, mean, stdv, xs=df_active.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot demonstrates where active learning has identified we should sample next, with the green dashed lines indicating which x-values should be sampled next. If this was a practical experiment, you now know where your next best bets be for improving your model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active learning when varying the number of requested points\n",
    "numpoints = [2, 3, 5]\n",
    "dfs_active = []\n",
    "for numpoint in numpoints:\n",
    "    df = tl.active_learn_campaign(campaign_id, numpoint)\n",
    "    dfs_active.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "for n, df in zip(numpoints, dfs_active):\n",
    "    plot_data(df_train, grid, mean, stdv, xs=df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active Learning loop\n",
    "df = df_train.copy()\n",
    "for i in range(4):\n",
    "    if i != 0:\n",
    "        tl.upload_dataset(df, dataset_id)\n",
    "        tl.train_campaign(params, campaign_id)\n",
    "    df_mean, df_stdv = tl.predict_campaign(df_test, campaign_id)\n",
    "    mean, stdv = df_mean[\"y\"].values, df_stdv[\"y\"].values\n",
    "    X = tl.active_learn_campaign(campaign_id, 1)\n",
    "    plot_data(df, grid, mean, stdv, xs=X.values)\n",
    "    y = model(X)\n",
    "    df = pd.concat([df, pd.DataFrame({\"X\": X.values[0], \"y\": y[0]})])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error messaging for active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Failing to specify any arguments:\")\n",
    "try:\n",
    "    df_active = tl.active_learn_campaign()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print()\n",
    "\n",
    "print(\"Failing to specify the number of points:\")\n",
    "try:\n",
    "    df_active = tl.active_learn_campaign(campaign_id)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print()\n",
    "\n",
    "print(\"Requesting a negative number of points:\")\n",
    "try:\n",
    "    df_active = tl.active_learn_campaign(campaign_id, -1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print()\n",
    "\n",
    "print(\"Requesting zero points:\")\n",
    "try:\n",
    "    df_active = tl.active_learn_campaign(campaign_id, 0)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing up "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers how to delete your trained model and dataset from the cloud. Note that you don't need to delete your model and data to rerun a campaign or dataset of the same name--if, say, you rerun the functions above as-is, with no name changes, will simply overwrite your existing model and dataset on the Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete campaign and dataset if necessary\n",
    "tl.delete_campaign(campaign_id, verbose=True)\n",
    "tl.delete_dataset(dataset_id, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twinlab-cloud-IBHCqXSr-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
