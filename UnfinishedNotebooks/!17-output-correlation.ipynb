{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we look at the ability of `twinLab` to model correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries: `numpy` allows us to efficiently manipulate arrays of numerical data; `pandas` gives us access to `DataFrames` which are a way of storing tabular data in `Python` and is the format used by `twinLab`. `matplotlib.pyplot` is used for plotting. `twinlab` is the machine-learning library we are using. Some of the libraries are renamed using `as` for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports\n",
    "import twinlab as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we create a dataset and upload it to the `twinLab` cloud.\n",
    "- The only input is $X$, which is linearly spaced.\n",
    "- The first output, $y_1$, is an array of `n` which is equal to $X$ plus a small amount of noise.\n",
    "- We define the second output $y_2$ as an array, where each element is the corresponding $y_3$ element divided by the corresponding $y_1$ element.\n",
    "   We give it the third output $y_3$ which is an array of `n` evenly spaced numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"degeneracy\"\n",
    "campaign_id = dataset_id\n",
    "random_seed = 43\n",
    "\n",
    "# Create training Data\n",
    "n = 101\n",
    "X = np.linspace(0, 1, n)\n",
    "y1 = X + np.random.normal(0, 0.05, n)\n",
    "y2 = np.random.rand(n)\n",
    "y3 = y1/y2\n",
    "train_data = pd.DataFrame({\"X\": X, \"y1\": y1, \"y2\": y2, \"y3\": y3})\n",
    "display(train_data)\n",
    "\n",
    "# Upload training data to twinLab\n",
    "tl.upload_dataset(train_data, dataset_id=dataset_id, verbose=True)\n",
    "tl.list_datasets(verbose=True)\n",
    "tl.query_dataset(dataset_id, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we set the parameters we are going to use to train the model; then we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "prediction_params = {\n",
    "    \"filename\": dataset_id,\n",
    "    \"inputs\" : [\"X\"],\n",
    "    \"outputs\": [\"y1\", \"y2\", \"y3\"],\n",
    "    \"test_train_ratio\": 1.,\n",
    "}\n",
    "\n",
    "# Training\n",
    "tl.train_campaign(prediction_params, campaign_id, verbose=True)\n",
    "tl.list_campaigns(verbose=True)\n",
    "tl.query_campaign(campaign_id, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell then creates the data we are going to use the model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = 1001\n",
    "input_dict = {\n",
    "    \"X\": np.linspace(0., 1., num_predictions).tolist(),\n",
    "}\n",
    "\n",
    "prediction_inputs = pd.DataFrame(input_dict)\n",
    "display(prediction_inputs)\n",
    "\n",
    "df_mean, df_std = tl.predict_campaign(prediction_inputs, campaign_id, verbose=True)\n",
    "display(df_mean)\n",
    "display(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then give these numbers to the model, and it generates what it thinks the three outputs should be. `df_mean` is the value it predicts. `df_std` is how uncertain the model is about that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the data on 3 graphs - one for X against $y_1$, one for X against $y_2$, and one for X against $y_3$.\n",
    "- The black dots on the graph are the training data we gave it. \n",
    "- The darkest blue line in the graph is the `df_mean` value.\n",
    "- The blue sections either side represent the range of uncertainty in the `df_mean` value.\n",
    "\n",
    "$y_1$ settles to around a value of 0.5.\n",
    "$y_2$'s average will increase the more numbers the model predicts, currently at around 2 - but with more data it would increase.\n",
    "$y_2$ also has some enourmously high values which occur whenever $y_1$ is a very tiny number, so the result of the division is very high. \n",
    "The third graph shows the model is good at predicting $y_3$, because the training data shows it is the same as the $X$ value it is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameters\n",
    "nsigs = [1, 2]\n",
    "color = \"blue\"\n",
    "alpha = 0.5\n",
    "plot_training_data = True\n",
    "plot_model_mean = True\n",
    "plot_model_bands = True\n",
    "\n",
    "# Plot results\n",
    "for Y, Ylabel in zip([\"y1\", \"y2\", \"y3\"], [\"$y_1$\", \"$y_2$\", \"$y_3$\"]):\n",
    "    grid = prediction_inputs[\"X\"]\n",
    "    mean = df_mean[Y]\n",
    "    err = df_std[Y]\n",
    "    if plot_model_bands:\n",
    "        label = \"Model prediction\"\n",
    "        plt.fill_between(grid, np.nan, np.nan, lw=0, color=color, alpha=alpha, label=label)\n",
    "        for isig, nsig in enumerate(nsigs):\n",
    "            plt.fill_between(grid, mean-nsig*err, mean+nsig*err, lw=0, color=color, alpha=alpha/(isig+1))\n",
    "    if plot_model_mean:\n",
    "        label = \"Model prediction\" if not plot_model_bands else None\n",
    "        plt.plot(grid, mean, color=color, alpha=alpha, label=label)\n",
    "    if plot_training_data:\n",
    "        plt.plot(train_data[\"X\"], train_data[Y], \".\", color=\"black\", label=\"Training data\")\n",
    "    plt.xlim((0., 1.))\n",
    "    plt.xlabel(\"$X$\")\n",
    "    plt.ylabel(Ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally remove our dataset and trained model from the `twinLab` cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete campaign and dataset (if desired)\n",
    "tl.delete_campaign(campaign_id, verbose=True)\n",
    "tl.delete_dataset(dataset_id, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twinlab-cloud-IBHCqXSr-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
