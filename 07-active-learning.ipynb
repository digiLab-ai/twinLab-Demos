{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Active Learning**\n",
    "\n",
    "Using `twinLab`, we can easily build a surrogate model of some real-world physical processes using limited experimental data. The discrepancy between this surrogate model and the real process is described by the (calibrated) uncertainty estimate produced our models. This way, inference activities while using the surrogate model can be supported by well-grounded evidence of the model *correctness*. In areas of high model uncertainty, one must then take care to not be overly trusting of the model output, which is especially important for safety-critical applications.\n",
    "\n",
    "In scenarios of high model uncertainty, one useful activity is to improve the model by collecting additional data. Here, **Active Learning** (also known as *Design of Experiments* or *Optimal Experimental Design*) as it is presented in `twinLab` answers the question of how best to obtain new experimental data, in order to build a surrogate which is *most accurate* while *requiring the least amount of data*. \n",
    "\n",
    "This notebook will cover: \n",
    "- Using Active Learning to iteratively improve a surrogate model in `twinLab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# twinLab import\n",
    "import twinlab as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-Dimensional example**\n",
    "\n",
    "We first begin with a one-dimensional example. Consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target function: forrester function\n",
    "def f(x, a=6, b=12):\n",
    "    return (a*x - 2)**2 * np.sin(b*x -4)\n",
    "\n",
    "X = np.linspace(0,1,100)[:,np.newaxis]\n",
    "y = f(X) # Arrange outputs as feature columns\n",
    "\n",
    "# Set up training data dataframe \n",
    "n_train = 6\n",
    "X_data = np.random.uniform(0, 1, size=n_train)\n",
    "y_data = f(X_data) + np.random.normal(scale=0.1, size=X_data.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5.5, 4.5))\n",
    "plt.plot(X,y, label=\"Target Function\")\n",
    "plt.scatter(X_data, y_data, color=\"red\", label=\"Training Data\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Quantity of Interest\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that the process to obtain this training data was so painstaking that we only managed 6 data samples from initial experiments. The (material or computational) costs may be prohibitively expensive to obtain many more experiments, but we would like to model this (very limited) experimental data nonetheless. We now use `tl.train_campaign` to build an initial surrogate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame({'X': X_data, 'y': y_data})\n",
    "df_test = pd.DataFrame({'X': X.flatten(), 'y': y.flatten()})\n",
    "\n",
    "# Define the name of the dataset\n",
    "dataset_id = \"Training_Data\"\n",
    "\n",
    "# Upload the dataset to the cloud\n",
    "tl.upload_dataset(df_train, dataset_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise campaign\n",
    "campaign_id = \"BasicGP\"\n",
    "\n",
    "campaign_params = {\n",
    "    \"dataset_id\": dataset_id,                   \n",
    "    \"inputs\": [\"X\"],                          \n",
    "    \"outputs\": [\"y\"],\n",
    "}                                        \n",
    "\n",
    "# Start a new campaign and train a surrogate model\n",
    "tl.train_campaign(campaign_params, campaign_id, verbose=True)\n",
    "\n",
    "# Plot inference results\n",
    "df_mean, df_stdev = tl.predict_campaign(df_test, campaign_id)\n",
    "y_mean, y_stdev = df_mean.values, df_stdev.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5.5, 4.5))\n",
    "plt.fill_between(X.flatten(), \n",
    "                 (y_mean - 2.58*y_stdev).flatten(), \n",
    "                 (y_mean + 2.58*y_stdev).flatten(), \n",
    "                 color='C0', alpha=0.25, label=\"Model Confidence Interval\")\n",
    "plt.plot(X, y_mean, c='C0', linewidth=2, label=\"Model Mean Prediction\")\n",
    "\n",
    "plt.scatter(df_train['X'], df_train['y'], color=\"red\", label=\"Training data\")\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "\n",
    "plt.plot(X, y, c='r', linewidth=2, linestyle='dashed', label=\"Target Function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad statistical model of 6 data samples, however we can see that the model does not line up exactly right with the true function in some areas. In this one-dimensional example, a human observer can pretty easily see that a new data sample should be obtained around `x=0.8`. However, when each data sample is so expensive to obtain, 'around' `x=...` may not be a good enough estimate. Furthermore, above two output dimensions it becomes very difficult for the human observer to visualise the model uncertainty. \n",
    "\n",
    "The short explanation of our Bayesian active learning algorithm is that it programmatically suggests the locations where the model uncertainty is highest, and suggest these as the new sample locations. First we will write a script defining an iteration of the following active learning loop: \n",
    "\n",
    "1. Build surrogate on currently available data\n",
    "2. Call `tl.active_learn_campaign(campaign_id, num_points=n)`, with `n` being the number of desired sample locations.\n",
    "3. Collect data at suggested locations (from experiment or simulator codes)\n",
    "4. Add new data to currently available data\n",
    "\n",
    "Steps (1) to (4) can be repeated infinitely, or more likely until the sampling budget is exhausted or the surrogate model passes some metric of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a helper function\n",
    "def active_learning(df_train, df_test=df_test, campaign_id=campaign_id, num_samples=1):\n",
    "    X_test = df_test[\"X\"].values\n",
    "\n",
    "    # Get current model predictions \n",
    "    df_mean, df_stdev = tl.predict_campaign(df_test, campaign_id)\n",
    "    y_mean, y_stdev = df_mean.values.flatten(), df_stdev.values.flatten()\n",
    "\n",
    "    # Compute optimal sample location(s)\n",
    "    df_new = tl.active_learn_campaign(campaign_id, num_points=num_samples)\n",
    "    print(\"Suggested Data Point(s):\")\n",
    "    display(df_new)\n",
    "    # Evaluate new data points   \n",
    "    df_new['y'] = f(df_new['X'].values) + np.random.normal(scale=0.1, size=num_samples)\n",
    "\n",
    "    # Plot current model predictions\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4.5))\n",
    "\n",
    "    # Plot the previous trained model and the suggested data point(s)\n",
    "    ax1.fill_between(X_test, y_mean+2.58*y_stdev, y_mean-2.58*y_stdev, color=\"C0\", alpha=0.25, label=\"Model Confidence Bound\")\n",
    "    ax1.plot(df_test[\"X\"], y_mean, color=\"C0\", label=\"Mean Prediction\")\n",
    "    ax1.scatter(df_train[\"X\"], df_train[\"y\"], color=\"red\", label=\"Training Data\")\n",
    "    ax1.plot(df_test[\"X\"], df_test[\"y\"], color=\"red\", linestyle=\"dashed\", label=\"Target Function\")\n",
    "    ax1.vlines(df_new[\"X\"], ymin=-10, ymax=20, color=\"gray\", linestyle=\"dashed\", alpha=0.8, label=\"Recommended sample location\")\n",
    "    ax1.set_title('Current Model')\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_xlabel(\"Input\")\n",
    "    ax1.set_ylabel(\"Quantity of Interest\")\n",
    "    # ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    # Train model with new data\n",
    "    df_train = pd.concat([df_train, df_new], ignore_index=True)\n",
    "\n",
    "    # Upload training dataframe\n",
    "    dataset_id = \"Training_Data\"\n",
    "\n",
    "    tl.upload_dataset(df_train, dataset_id, verbose=True)\n",
    "\n",
    "    campaign_params = {\n",
    "    \"dataset_id\": dataset_id,                \n",
    "    \"inputs\": [\"X\"],                           \n",
    "    \"outputs\": [\"y\"],\n",
    "    }                                        \n",
    "\n",
    "    # Start a new campaign and train a surrogate model\n",
    "    tl.train_campaign(campaign_params, campaign_id, verbose=True)\n",
    "    \n",
    "    # Plot model output with new data point(s)\n",
    "    df_mean, df_stdev = tl.predict_campaign(df_test, campaign_id)\n",
    "    y_mean, y_stdev = df_mean.values.flatten(), df_stdev.values.flatten()\n",
    "    ax2.fill_between(df_test[\"X\"], y_mean+2.58*y_stdev, y_mean-2.58*y_stdev, color=\"C0\", alpha=0.25, label=\"Confidence Bound\")\n",
    "    ax2.plot(df_test[\"X\"], y_mean, color=\"C0\", label=\"Mean Prediction\")\n",
    "    ax2.scatter(df_train[\"X\"][:-num_samples], df_train[\"y\"][:-num_samples], color=\"red\", label=\"Training Data\")\n",
    "    ax2.plot(df_test[\"X\"], df_test[\"y\"], color=\"red\", linestyle=\"dashed\", label=\"Target Function\")\n",
    "    ax2.scatter(df_train[\"X\"][len(df_train)-num_samples:], df_train[\"y\"][len(df_train)-num_samples:], color=\"gray\", label=\"Recommended Sample\")\n",
    "    ax2.set_title('Next Model')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_xlabel(\"Input\")\n",
    "    ax2.set_ylabel(\"Quantity of Interest\")\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return df_train, campaign_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this script, we can now run an iteration of the active learning loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, campaign_id = active_learning(df_train, df_test=df_test, campaign_id=campaign_id, num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm suggests that the next location to sample is `x=0.765`. Already, the model is much better than before. If we can afford to, we may request another sample location, or stop here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the campaign and dataset\n",
    "tl.delete_campaign(\"BasicGP\")\n",
    "tl.delete_dataset(\"Training_Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-Dimensional Example**\n",
    "\n",
    "Even in the previous one-dimensional example, it is difficult for a human observer to accurately determine the location of maximal model uncertainty. In this example, we demonstrate the difficulty of scaling up the 'human expert' with a two-dimensional example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a non-linear, 2D function\n",
    "def target_function(x, y):\n",
    "    # Branin function\n",
    "    a = 1\n",
    "    b = 5.1 / (4 * np.pi ** 2)\n",
    "    c = 5 / np.pi\n",
    "    r = 6\n",
    "    s = 10\n",
    "    t = 1 / (8 * np.pi)\n",
    "    return (\n",
    "        a * (y - b * x ** 2 + c * x - r) ** 2\n",
    "        + s * (1 - t) * np.cos(x)\n",
    "        + s\n",
    "    )\n",
    "\n",
    "\n",
    "# Plotting function\n",
    "def plot_function(\n",
    "    xx, yy, zz,  # Background grid\n",
    "    zz_pred=None, zz_stdv=None, # Mean and uncertainty in model function\n",
    "    x_org=None, y_org=None, z_org=None,  # Original training data\n",
    "    x_new=None, y_new=None, z_new=None,  # New training data\n",
    "    x_upd=None, y_upd=None,  # Proposed candidate points\n",
    "    show_truth=True,  # Show true function\n",
    "    ):\n",
    "\n",
    "    cmap_fn, cmap_std = 'YlGn', 'Blues'\n",
    "    alpha_fn, alpha_std = 0.8, 1.\n",
    "    levels = 20\n",
    "    color_org, color_new, color_upd = 'black', 'blue', 'red'\n",
    "    marker_org, marker_new, marker_upd = 'x', 'x', 'x'\n",
    "    marker_col = 'o'\n",
    "    vmin, vmax = zz.min(), zz.max()\n",
    "    figx, figy = 4, 4\n",
    "\n",
    "    # Set layout\n",
    "    if zz_pred is None and zz_stdv is None:\n",
    "        rows, cols = 1, 1\n",
    "    else:\n",
    "        rows, cols = 1, 3\n",
    "    _, axs = plt.subplots(rows, cols, figsize=(cols*figx, rows*figy))\n",
    "\n",
    "    if rows == 1 and cols == 1:\n",
    "        axs = np.array([axs])\n",
    "    axs = np.atleast_2d(axs)\n",
    "    for ax in axs.flatten():\n",
    "        ax.set_xticks([]), ax.set_yticks([])\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "    # Plot true function\n",
    "    if show_truth:\n",
    "        axs[0, 0].set_title('True function')\n",
    "        axs[0, 0].contourf(xx, yy, zz, cmap=cmap_fn, levels=levels, alpha=alpha_fn, vmin=vmin, vmax=vmax)\n",
    "    else:\n",
    "        axs[0, 0].set_title('True function (unknown)')\n",
    "        axs[0, 0].set_facecolor('lightgrey')\n",
    "    \n",
    "    if x_org is not None and y_org is not None and z_org is not None: # Plot original points\n",
    "        if show_truth:\n",
    "            axs[0, 0].scatter(x_org, y_org, c=color_org, marker=marker_org, label='Original samples')\n",
    "        else:\n",
    "            axs[0, 0].scatter(x_org, y_org, c=z_org, marker=marker_col, cmap=cmap_fn, vmin=vmin, vmax=vmax, label='Original samples')\n",
    "        axs[0, 0].legend(loc=\"upper left\")\n",
    "\n",
    "    if x_new is not None and y_new is not None and z_new is not None: # Plot new points\n",
    "        color = color_new if show_truth else z_new\n",
    "        marker = marker_new if show_truth else marker_col\n",
    "        axs[0, 0].scatter(x_new, y_new, c=color, marker=marker, cmap=cmap_fn, vmin=vmin, vmax=vmax)\n",
    "        axs[0, 0].legend(loc=\"upper left\")\n",
    "\n",
    "    # Plot mean prediction\n",
    "    if zz_pred is not None and zz_stdv is not None:\n",
    "        axs[0, 1].set_title('Model function')\n",
    "        axs[0, 1].contourf(xx, yy, zz_pred, cmap=cmap_fn, levels=levels, alpha=alpha_fn, vmin=vmin, vmax=vmax)\n",
    "        axs[0, 2].set_title('Model uncertainty')\n",
    "        axs[0, 2].contourf(xx, yy, zz_stdv, cmap=cmap_std, levels=levels, alpha=alpha_std)\n",
    "        if x_org is not None: # Plot original points\n",
    "            axs[0, 1].scatter(x_org, y_org, c=color_org, marker=marker_org)\n",
    "            axs[0, 2].scatter(x_org, y_org, c=color_org, marker=marker_org)\n",
    "            if rows == 2:\n",
    "                axs[1, 1].scatter(x_org, y_org, c=color_org, marker=marker_org)\n",
    "                axs[1, 2].scatter(x_org, y_org, c=color_org, marker=marker_org)\n",
    "        if x_new is not None: # Plot new points\n",
    "            axs[0, 1].scatter(x_new, y_new, c=color_new, marker=marker_new, label='New samples')\n",
    "            axs[0, 2].scatter(x_new, y_new, c=color_new, marker=marker_new)\n",
    "            if rows == 2:\n",
    "                axs[1, 1].scatter(x_new, y_new, c=color_new, marker=marker_new)\n",
    "                axs[1, 2].scatter(x_new, y_new, c=color_new, marker=marker_new)\n",
    "            axs[0, 1].legend(loc=\"upper left\")\n",
    "        if x_upd is not None: # Plot candidate points\n",
    "            axs[0, 2].scatter(x_upd, y_upd, c=color_upd, marker=marker_upd, label='Recommended future samples')\n",
    "            axs[0, 2].legend(loc=\"upper left\")\n",
    "            if rows == 2:\n",
    "                axs[1, 2].scatter(x_upd, y_upd, c=color_upd, marker=marker_upd)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the random-number generator\n",
    "seed = 4\n",
    "np.random.seed(seed)\n",
    "nmesh = 101\n",
    "xmin, xmax = -5, 10\n",
    "ymin, ymax = 0, 15\n",
    "\n",
    "# Generate a grid of points and evaluate the function at each point\n",
    "x, y = np.linspace(xmin, xmax, nmesh), np.linspace(ymin, ymax, nmesh)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "zz = target_function(xx, yy)\n",
    "\n",
    "# Generate noisy train data\n",
    "n_train = 9\n",
    "err = 0.1\n",
    "x_train = np.random.uniform(xmin, xmax, n_train)\n",
    "y_train = np.random.uniform(ymin, ymax, n_train)\n",
    "z_train = np.random.normal(target_function(x_train, y_train), err, n_train)\n",
    "\n",
    "# Number of new data points to generate each iteration\n",
    "n_candidates = 3\n",
    "\n",
    "# Plot the data points\n",
    "plot_function(xx, yy, zz, x_org=x_train, y_org=y_train)\n",
    "\n",
    "# Convert to dataframes\n",
    "df_train = pd.DataFrame({'x': x_train.flatten(), 'y': y_train.flatten(), 'z': z_train.flatten()})\n",
    "df_test = pd.DataFrame({'x': xx.flatten(), 'y': yy.flatten(), 'z': zz.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the dataset\n",
    "dataset_id = \"2DActive_Data\"\n",
    "\n",
    "# Upload the dataset to the cloud\n",
    "tl.upload_dataset(df_train, dataset_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise campaign\n",
    "campaign_id = \"2DActiveGP\"\n",
    "\n",
    "campaign_params = {\n",
    "    \"dataset_id\": dataset_id,                  \n",
    "    \"inputs\": ['x','y'],                        \n",
    "    \"outputs\": ['z'],\n",
    "}                                        \n",
    "\n",
    "# Start a new campaign and train a surrogate model\n",
    "tl.train_campaign(campaign_params, campaign_id, verbose=True)\n",
    "\n",
    "# Plot inference results\n",
    "df_mean, df_stdev = tl.predict_campaign(df_test, campaign_id)\n",
    "y_mean, y_stdev = df_mean.values, df_stdev.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our loop function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_sample(campaign_id, df_train, df_test, n_candidates, df_new=None, show_truth=True):\n",
    "\n",
    "    # Plot inference results\n",
    "    df_mean, df_stdev = tl.predict_campaign(df_test, campaign_id)\n",
    "    z_mean, z_stdv = df_mean.values, df_stdev.values\n",
    "\n",
    "    x_org, y_org, z_org = df_train['x'].values, df_train['y'].values, df_train['z'].values\n",
    "    if df_new is not None:\n",
    "        x_new, y_new = df_new['x'].values, df_new['y'].values\n",
    "        z_new = df_new['z'].values\n",
    "    else:\n",
    "        x_new, y_new, z_new = None, None, None\n",
    "\n",
    "    # Get new candidate points\n",
    "    candidates = tl.active_learn_campaign(campaign_id, num_points=n_candidates).values\n",
    "    x_upd, y_upd = candidates[:, 0], candidates[:, 1]\n",
    "\n",
    "    # Plot sample location\n",
    "    plot_function(xx, yy, zz, \n",
    "                  z_mean.reshape((nmesh, nmesh)), z_stdv.reshape((nmesh, nmesh)),\n",
    "                  x_org=x_org, y_org=y_org, z_org=z_org,\n",
    "                  x_new=x_new, y_new=y_new, z_new=z_new,\n",
    "                  x_upd=x_upd, y_upd=y_upd,\n",
    "                  show_truth=show_truth,\n",
    "                  )\n",
    "\n",
    "    # Sample at new location(s)\n",
    "    z_upd = np.random.normal(target_function(x_upd, y_upd), 0.1)\n",
    "\n",
    "    # Add new sample(s) to training data\n",
    "    df_upd = pd.DataFrame({'x': x_upd, 'y': y_upd, 'z': z_upd})\n",
    "    df_train = pd.concat([df_train, df_upd])\n",
    "\n",
    "\n",
    "\n",
    "    tl.upload_dataset(df_train, \"New_Points\")\n",
    "\n",
    "    # Initialise campaign\n",
    "    campaign_params = {\n",
    "        \"dataset_id\": \"New_Points\",                  \n",
    "        \"inputs\": ['x','y'],                       \n",
    "        \"outputs\": ['z'],\n",
    "    }                                        \n",
    "\n",
    "    #   Start a new campaign and train a surrogate model\n",
    "    tl.train_campaign(campaign_params, campaign_id, verbose=True)\n",
    "    return campaign_id, df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and request the first set of candidate points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a first set of candidates\n",
    "gp_campaign, _df_train = suggest_sample(\"2DActiveGP\", df_train, df_test, n_candidates=n_candidates, show_truth=False)\n",
    "df_new = _df_train[-3:]\n",
    "df_train = _df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then request another set of points and view the impact of the last set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request another set of candidates\n",
    "gp_campaign, df_train = suggest_sample(\"2DActiveGP\", df_train, df_test, n_candidates=n_candidates, df_new=df_new, show_truth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Error messaging for active learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Failing to specify any arguments:\")\n",
    "try:\n",
    "    df_active = tl.active_learn_campaign()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print()\n",
    "\n",
    "print(\"Failing to specify the number of points:\")\n",
    "try:\n",
    "    df_active = tl.active_learn_campaign(campaign_id)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print()\n",
    "\n",
    "print(\"Requesting a negative number of points:\")\n",
    "try:\n",
    "    df_active = tl.active_learn_campaign(campaign_id, -1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print()\n",
    "\n",
    "print(\"Requesting zero points:\")\n",
    "try:\n",
    "    df_active = tl.active_learn_campaign(campaign_id, 0)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete campaign and dataset\n",
    "tl.delete_campaign(campaign_id)\n",
    "\n",
    "tl.delete_dataset(\"2DActive_Data\")\n",
    "tl.delete_dataset(\"Training_Data\")\n",
    "tl.delete_dataset(\"New_Points\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twinlab-T1BT2Sfn-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
