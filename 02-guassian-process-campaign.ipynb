{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gaussian Process Campaign**\n",
    "\n",
    "This notebook contains basic working examples of fitting and running inference using Gaussian Processes (GPs) as the base estimator model in `twinLab`. \n",
    "\n",
    "An in-depth introductory review of GPs can be found at [Rasmussen and Williams (2006)](https://gaussianprocess.org/gpml/). \n",
    "\n",
    "This notebook will cover:\n",
    "- Basic syntax\n",
    "- Detrending data\n",
    "- Specifying a covarance module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# twinLab import\n",
    "import twinlab as tl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem Formulation**\n",
    "As a start, we will solve a simple regression problem with one input and one output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true function\n",
    "def oscillator(x):\n",
    "    return np.cos((x - 5) / 2) ** 2 * x * 2\n",
    "\n",
    "X = np.linspace(-15,15,100)[:,np.newaxis]\n",
    "y = oscillator(X) # Arrange outputs as feature columns\n",
    "\n",
    "n_data = 200\n",
    "X_data = np.random.uniform(-10, 10, size=n_data)\n",
    "y_data = oscillator(X_data) + np.random.normal(scale=0.2, size=X_data.shape)\n",
    "\n",
    "plt.plot(X,y)\n",
    "plt.scatter(X_data, y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets must be data presented as a `pandas.DataFrame` object, or a filepaths which points to a csv file that can be parsed to a `pandas.DataFrame` object. **Both must be formatted with clearly labelled columns.** Here, we will label the input (predictor) variable `x` and the output variable `y`. In `twinlab`, data is expected to be in column-feature format, meaning each row represents a single data sample, and each column represents a data feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "df = pd.DataFrame({'x': X_data, 'y': y_data})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the dataset\n",
    "dataset_id = \"BasicGP_Data\"\n",
    "\n",
    "# Upload the dataset to the cloud\n",
    "tl.upload_dataset(df, dataset_id, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Basic Campaign Workflow**\n",
    "\n",
    "By default, a `Campaign` object will try to fit a Gaussian Process with a Constant Mean and a Matern 5/2 covariance kernel to the data. This is done with the basic syntax shown below. \n",
    "\n",
    "**Note**: the `train_test_ratio` parameter determines how many data samples in `df` to use for fitting the model. E.g. if `train_test_ratio=0.4`, then 40% of the samples in `df` are used for fitting, and the remaining data is used for testing. The data reserved for testing is used to assess the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise campaign\n",
    "campaign_id = \"BasicGP\"\n",
    "\n",
    "campaign_params = {\n",
    "    \"dataset_id\": dataset_id,                   # This points the campaign to the uploaded dataset\n",
    "    \"inputs\": [\"x\"],                            # Using the datasets column headers define the input and output data\n",
    "    \"outputs\": [\"y\"],\n",
    "    \"test_train_ratio\": 0.75,                   # Determine how much data is used for training, here 75% is used to train the model  \n",
    "    \"estimator\": \"gaussian_process_regression\", # and 25% is used to test it.\n",
    "}                                        \n",
    "\n",
    "# Start a new campaign and train a surrogate model\n",
    "tl.train_campaign(campaign_params, campaign_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inference results\n",
    "df_mean, df_stdev = tl.predict_campaign(pd.DataFrame(X, columns=[\"x\"]), campaign_id)\n",
    "y_mean, y_stdev = df_mean.values, df_stdev.values\n",
    "\n",
    "plt.fill_between(X.flatten(), \n",
    "                 (y_mean - 1.96*y_stdev).flatten(), \n",
    "                 (y_mean + 1.96*y_stdev).flatten(), \n",
    "                 color='k', alpha=0.1)\n",
    "\n",
    "plt.scatter(df['x'], df['y'], alpha=0.5, label='Training Data')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "\n",
    "plt.plot(X, y, c='r', linewidth=2, label='True Function')\n",
    "plt.plot(X, y_mean, c='k', linewidth=2, linestyle='dashed', label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Detrending Data**\n",
    "\n",
    "A standard data processing task before fitting a model to data is (linear) *detrending*: here, the linear non-stationary information about the data is subtracted from the data. This may help with subsequent model fitting. \n",
    "\n",
    "In `Campaign`, this is achieved simply with the keyword `detrend` provided to the `estimator_kwargs` dictionary, inside the `campaign_params`, during initialisation. This results in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise campaign\n",
    "campaign_id = \"DetrendingGP\"\n",
    "\n",
    "campaign_params = {\n",
    "    \"dataset_id\": dataset_id,                   # This points the campaign to the uploaded dataset\n",
    "    \"inputs\": [\"x\"],                            # Using the datasets column headers define the input and output data\n",
    "    \"outputs\": [\"y\"],\n",
    "    \"test_train_ratio\": 0.75,                   # Determine how much data is used for training, here 75% is used to train the model  \n",
    "    \"estimator\": \"gaussian_process_regression\", # and 25% is used to test it.\n",
    "    'estimator_kwargs': {\n",
    "        'detrend': True\n",
    "    }\n",
    "}                                        \n",
    "\n",
    "# Start a new campaign and train a surrogate model\n",
    "tl.train_campaign(campaign_params, campaign_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inference results\n",
    "df_mean, df_stdev = tl.predict_campaign(pd.DataFrame(X, columns=[\"x\"]), campaign_id)\n",
    "y_mean, y_stdev = df_mean.values, df_stdev.values\n",
    "\n",
    "plt.fill_between(X.flatten(), \n",
    "                 (y_mean - 1.96*y_stdev).flatten(), \n",
    "                 (y_mean + 1.96*y_stdev).flatten(), \n",
    "                 color='k', alpha=0.1)\n",
    "\n",
    "plt.scatter(df['x'], df['y'], alpha=0.5, label='Training Data')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "\n",
    "plt.plot(X, y, c='r', linewidth=2, label='True Function')\n",
    "plt.plot(X, y_mean, c='k', linewidth=2, linestyle='dashed', label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference in the mean prediction of the detrended model compared to the standard model without detrending (from the previous section). The prediction is automatically re-trended during inference, so no need for any other operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specifying a Covariance Kernel**\n",
    "\n",
    "In some cases, it may be desirable to specify a covariance module for the GP. The user may have some belief that the process is linear in nature, or may contain a combination of periodic signals at different length-scales, or may want to specifically account for short-time noise in the data. \n",
    "\n",
    "`Campaign` allows the user to do this, via the `covar_module` keyword provided to the `estimator_kwargs` dictionary. Currently, the kernels are provided as 3-character uppercase strings. The list of possible kernels are: \n",
    "- `\"LIN\"`: Linear kernel\n",
    "- `\"M12\"`: Matern 1/2 kernel\n",
    "- `\"M32\"`: Matern 3/2 kernel\n",
    "- `\"M52\"`: Matern 5/2 kernel\n",
    "- `\"PER\"`: Periodic kernel\n",
    "- `\"RBF\"`: Radial Basis Function kernel\n",
    "- `\"RQF\"`: Rational Quadratic Kernel\n",
    "\n",
    "Additionally, the kernels can be [composed](https://www.cs.toronto.edu/~duvenaud/cookbook/) using brackets and `+` or `*` string operators. For example:\n",
    "- `\"LIN+PER\"`\n",
    "- `\"(M52*PER)+RQF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise campaign\n",
    "campaign_id = \"LinearGP\"\n",
    "\n",
    "campaign_params = {\n",
    "    \"dataset_id\": dataset_id,                   # This points the campaign to the uploaded dataset\n",
    "    \"inputs\": [\"x\"],                            # Using the datasets column headers define the input and output data\n",
    "    \"outputs\": [\"y\"],\n",
    "    \"test_train_ratio\": 0.75,                   # Determine how much data is used for training, here 75% is used to train the model  \n",
    "    \"estimator\": \"gaussian_process_regression\", # and 25% is used to test it.\n",
    "    'estimator_kwargs': {\n",
    "        'covar_module': \"LIN\"\n",
    "    }\n",
    "}                                        \n",
    "\n",
    "# Start a new campaign and train a surrogate model\n",
    "tl.train_campaign(campaign_params, campaign_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inference results\n",
    "df_mean, df_stdev = tl.predict_campaign(pd.DataFrame(X, columns=[\"x\"]), campaign_id)\n",
    "y_mean, y_stdev = df_mean.values, df_stdev.values\n",
    "\n",
    "plt.fill_between(X.flatten(), \n",
    "                 (y_mean - 1.96*y_stdev).flatten(), \n",
    "                 (y_mean + 1.96*y_stdev).flatten(), \n",
    "                 color='k', alpha=0.1)\n",
    "\n",
    "plt.scatter(df['x'], df['y'], alpha=0.5, label='Training Data')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "\n",
    "plt.plot(X, y, c='r', linewidth=2, label='True Function')\n",
    "plt.plot(X, y_mean, c='k', linewidth=2, linestyle='dashed', label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete campaigns and dataset\n",
    "tl.delete_campaign(\"BasicGP\")\n",
    "tl.delete_campaign(\"DetrendingGP\")\n",
    "tl.delete_campaign(\"LinearGP\")\n",
    "\n",
    "tl.delete_dataset(dataset_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a73b81f77433a07a5451f1269b58376229ced6e8222af986634b7460575721ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
