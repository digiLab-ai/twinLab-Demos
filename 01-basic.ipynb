{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we import 'twinlab' for the first time and create a basic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries: `numpy` allows us to efficiently manipulate arrays of numerical data; `pandas` gives us access to `DataFrames` which are a way of storing tabular data in `Python` and is the format used by `twinLab`. `matplotlib.pyplot` is used for plotting. `twinlab` is the machine-learning library we are using. Some of the libraries are renamed using `as` for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports\n",
    "import twinlab as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `twinLab-Demos` repository contains a number of sample datasets; in this cell we define the file paths for a basic dataset which will be used for this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "campaign_dir = os.path.join(\"resources\", \"campaigns\", \"basic\")\n",
    "datasets_dir = os.path.join(\"resources\", \"datasets\")\n",
    "file_train = os.path.join(datasets_dir, \"basic.csv\")\n",
    "file_eval = os.path.join(campaign_dir, \"eval.csv\")\n",
    "file_params = os.path.join(campaign_dir, \"params.json\")\n",
    "\n",
    "# Campaign id\n",
    "dataset_id = \"basic\"\n",
    "campaign_id = \"basic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `tl.upload_dataset` we upload the dataset to the cloud so it can be used to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset to the cloud\n",
    "tl.upload_dataset(file_train, dataset_id, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that our dataset was succefully upload we can use `tl.list_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List datasets\n",
    "_ = tl.list_datasets(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train our surrogate model, `tl.train_campaign` can be used to train any model type availabel in `twinLab`, the type can be determined using the `campaign_id`. The keyword `file_params` is used to define the dataset that the model is trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new campaign and train a surrogate model\n",
    "tl.train_campaign(file_params, campaign_id, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use `tl.list_campaigns` to check that our model has been trained and exists in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List campaigns\n",
    "_ = tl.list_campaigns(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tl.query_campaign` function can be used to view all the parameters of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the status of a campaign\n",
    "_ = tl.query_campaign(campaign_id, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `tl.predict_campaign` with our `campaign_id` we can use our trained model to calculate a prediction and corresponding standard deviation. These results are saved to a csv file with a path defined by `file_eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained emulator\n",
    "df_mean, df_std = tl.predict_campaign(file_eval, campaign_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and evaluation data (for plotting)\n",
    "df_train = pd.read_csv(file_train)\n",
    "df_eval = pd.read_csv(file_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameters\n",
    "nsigs = [1, 2]\n",
    "# nsigs = [0.674, 1.960, 2.576]\n",
    "color = \"blue\"\n",
    "alpha = 0.5\n",
    "plot_training_data = True\n",
    "plot_model_mean = True\n",
    "plot_model_bands = True\n",
    "\n",
    "# Plot results\n",
    "grid = df_eval[\"x\"]\n",
    "mean = df_mean[\"y\"]\n",
    "err = df_std[\"y\"]\n",
    "if plot_model_bands:\n",
    "    label = r\"Model prediction\"\n",
    "    plt.fill_between(grid, np.nan, np.nan, lw=0, color=color, alpha=alpha, label=label)\n",
    "    for isig, nsig in enumerate(nsigs):\n",
    "        plt.fill_between(grid, mean-nsig*err, mean+nsig*err, lw=0, color=color, alpha=alpha/(isig+1))\n",
    "if plot_model_mean:\n",
    "    label = r\"Model prediction\" if not plot_model_bands else None\n",
    "    plt.plot(grid, mean, color=color, alpha=alpha, label=label)\n",
    "if plot_training_data:\n",
    "    plt.plot(df_train[\"x\"], df_train[\"y\"], \".\", color=\"black\", label=\"Training data\")\n",
    "plt.xlim((0.0, 1.0))\n",
    "plt.xlabel(r\"$X$\")\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are finished with our twinLab campaign we can delete the model and dataset from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete campaign and dataset (if desired)\n",
    "tl.delete_campaign(campaign_id, verbose=True)\n",
    "tl.delete_dataset(dataset_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
