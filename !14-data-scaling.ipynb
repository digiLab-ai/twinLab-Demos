{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we look at the ability of `twinLab` to scale datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries: `numpy` allows us to efficiently manipulate arrays of numerical data; `pandas` gives us access to `DataFrames` which are a way of storing tabular data in `Python` and is the format used by `twinLab`. `matplotlib.pyplot` is used for plotting. `twinlab` is the machine-learning library we are using. Some of the libraries are renamed using `as` for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports\n",
    "import twinlab as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_id = \"data-scaling\"\n",
    "campaign_id = dataset_id\n",
    "err_sig = 0.25\n",
    "n_train = 100\n",
    "n_eval = 101\n",
    "random_seed = 42\n",
    "n_cycle = 2\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(2*np.pi*x*n_cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we generate our training dataset using the function and constants above. By uploading the dataset via tl.upload_dataset(), with the keyword argument `data-scaling`, the data will be scaled to values bewtween 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the random-number generator\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Training Data\n",
    "X = np.random.rand(n_train)\n",
    "y = f(X)+np.random.normal(0., err_sig, n_train)\n",
    "df_train = pd.DataFrame({'X': X, 'y': y})\n",
    "display(df_train)\n",
    "tl.upload_dataset(df_train, dataset_id, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation data\n",
    "eval = {\"X\": np.linspace(0, 1, n_eval)}\n",
    "df_test = pd.DataFrame(eval)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "params = {\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"inputs\" : [\"X\"],\n",
    "    \"outputs\": [\"y\"],\n",
    "    \"test_train_ratio\": 1.,\n",
    "}\n",
    "\n",
    "# Plot parameters\n",
    "grid = df_test[\"X\"].values\n",
    "alpha_fill = 0.25\n",
    "ratios = [0.1, 0.2, 0.4, 0.8]\n",
    "nrow, ncol = 2, 2\n",
    "figx, figy = 4, 3\n",
    "\n",
    "# Loop over different error in data and plot\n",
    "plt.subplots(nrow, ncol, figsize=(ncol*figx, nrow*figy))\n",
    "for iplot, ratio in enumerate(ratios):\n",
    "\n",
    "    # Train model\n",
    "    n = int(ratio*len(df_train))\n",
    "    print(\"Number of data points used for training:\", n)\n",
    "    params[\"test_train_ratio\"] = ratio\n",
    "    tl.train_campaign(params, campaign_id, verbose=True)\n",
    "\n",
    "    # Predict\n",
    "    df_mean, df_std = tl.predict_campaign(df_test, campaign_id)\n",
    "    mean, err = df_mean[\"y\"].values, df_std[\"y\"].values\n",
    "\n",
    "    # Plot\n",
    "    color = f\"C{iplot}\"\n",
    "    plt.subplot(nrow, ncol, iplot+1)\n",
    "    plt.plot(df_train[\"X\"][:n], df_train[\"y\"][:n], \".\", color=\"black\")\n",
    "    plt.plot(grid, mean, \"-\", color=color, label=f\"N = {n}\")\n",
    "    for nsig in [1, 2]:\n",
    "        plt.fill_between(grid, mean-nsig*err, mean+nsig*err, lw=0, color=color, alpha=alpha_fill)\n",
    "    # plt.xlabel(\"X\"); plt.ylabel(\"y\")\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.ylim((-1.5, 1.5))\n",
    "    plt.legend()\n",
    "\n",
    "# Finalize plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete campaign and dataset if necessary\n",
    "tl.delete_campaign(campaign_id, verbose=True)\n",
    "tl.delete_dataset(dataset_id, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twinlab-cloud-IBHCqXSr-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
